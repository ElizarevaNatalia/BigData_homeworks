{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: /user/st085838/\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-930e736913b2>:26 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-930e736913b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#some needed objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    337\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-930e736913b2>:26 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import sklearn\n",
    "import socket\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "print('user:', os.environ['JUPYTERHUB_SERVICE_PREFIX'])\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return \"{}proxy/{}/jobs/\".format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "# small fix to enable UI views\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "# spark configurtion in local regime \n",
    "conf = SparkConf().set('spark.master', 'local[*]').set('spark.driver.memory', '8g')\n",
    "\n",
    "#some needed objects\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"file:///home/jovyan/shared/lectures_folder/84-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read text as a dataframe\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "dataframe= sc.textFile(f\"{filepath}\")\\\n",
    "     .map(lambda x: (x,))\\\n",
    "     .toDF()\\\n",
    "     .select(F.col(\"_1\").alias(\"text\"))\\\n",
    "     .withColumn(\"doc_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                text|doc_id|\n",
      "+--------------------+------+\n",
      "|The Project Guten...|     0|\n",
      "|                    |     1|\n",
      "|This eBook is for...|     2|\n",
      "|most other parts ...|     3|\n",
      "|whatsoever. You m...|     4|\n",
      "|of the Project Gu...|     5|\n",
      "|www.gutenberg.org...|     6|\n",
      "|will have to chec...|     7|\n",
      "|   using this eBook.|     8|\n",
      "|                    |     9|\n",
      "| Title: Frankenstein|    10|\n",
      "|       or, The Mo...|    11|\n",
      "|                    |    12|\n",
      "|Author: Mary Woll...|    13|\n",
      "|                    |    14|\n",
      "|Release Date: 31,...|    15|\n",
      "|[Most recently up...|    16|\n",
      "|                    |    17|\n",
      "|   Language: English|    18|\n",
      "|                    |    19|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7743"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for filtering out non-letters, creating list of words\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType\n",
    "import string\n",
    "import re\n",
    "\n",
    "def process_string(data):\n",
    "    non_letters_removed = re.sub(r'[^a-zA-Z]', ' ', data)\n",
    "    words = non_letters_removed.lower().split(\" \") \n",
    "    return list(filter(lambda x: len(x) > 0, words))\n",
    "    \n",
    "process_string_udf = udf(lambda z: process_string(z), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out empty strings,apply function written above\n",
    "by_words = dataframe\\\n",
    "    .select(process_string_udf(F.col(\"text\")).alias(\"by_words\"))\\\n",
    "    .where(F.size(F.col(\"by_words\")) > 0)\\\n",
    "    .withColumn(\"doc_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            by_words|doc_id|\n",
      "+--------------------+------+\n",
      "|[the, project, gu...|     0|\n",
      "|[this, ebook, is,...|     1|\n",
      "|[most, other, par...|     2|\n",
      "|[whatsoever, you,...|     3|\n",
      "|[of, the, project...|     4|\n",
      "|[www, gutenberg, ...|     5|\n",
      "|[will, have, to, ...|     6|\n",
      "|[using, this, ebook]|     7|\n",
      "|[title, frankenst...|     8|\n",
      "|[or, the, modern,...|     9|\n",
      "|[author, mary, wo...|    10|\n",
      "|[release, date, e...|    11|\n",
      "|[most, recently, ...|    12|\n",
      "| [language, english]|    13|\n",
      "|[character, set, ...|    14|\n",
      "|[produced, by, ju...|    15|\n",
      "|[further, correct...|    16|\n",
      "|[start, of, the, ...|    17|\n",
      "|      [frankenstein]|    18|\n",
      "|[or, the, modern,...|    19|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "by_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column with separate words in each row\n",
    "by_words_count = by_words\\\n",
    "     .withColumn('word',(F.explode(F.col(\"by_words\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------+\n",
      "|            by_words|doc_id|          word|\n",
      "+--------------------+------+--------------+\n",
      "|[the, project, gu...|     0|           the|\n",
      "|[the, project, gu...|     0|       project|\n",
      "|[the, project, gu...|     0|     gutenberg|\n",
      "|[the, project, gu...|     0|         ebook|\n",
      "|[the, project, gu...|     0|            of|\n",
      "|[the, project, gu...|     0|  frankenstein|\n",
      "|[the, project, gu...|     0|            by|\n",
      "|[the, project, gu...|     0|          mary|\n",
      "|[the, project, gu...|     0|wollstonecraft|\n",
      "|[the, project, gu...|     0|        godwin|\n",
      "|[the, project, gu...|     0|       shelley|\n",
      "|[this, ebook, is,...|     1|          this|\n",
      "|[this, ebook, is,...|     1|         ebook|\n",
      "|[this, ebook, is,...|     1|            is|\n",
      "|[this, ebook, is,...|     1|           for|\n",
      "|[this, ebook, is,...|     1|           the|\n",
      "|[this, ebook, is,...|     1|           use|\n",
      "|[this, ebook, is,...|     1|            of|\n",
      "|[this, ebook, is,...|     1|        anyone|\n",
      "|[this, ebook, is,...|     1|      anywhere|\n",
      "+--------------------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "by_words_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate each term frequency in each document \n",
    "by_words_tf= by_words_count\\\n",
    "        .groupBy('doc_id', 'word').count()\\\n",
    "        .orderBy(F.col(\"doc_id\"))\\\n",
    "        .withColumnRenamed(\"count\", 'tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---+\n",
      "|doc_id|          word| tf|\n",
      "+------+--------------+---+\n",
      "|     0|          mary|  1|\n",
      "|     0|        godwin|  1|\n",
      "|     0|wollstonecraft|  1|\n",
      "|     0|           the|  1|\n",
      "|     0|     gutenberg|  1|\n",
      "|     0|         ebook|  1|\n",
      "|     0|  frankenstein|  1|\n",
      "|     0|            of|  1|\n",
      "|     0|       project|  1|\n",
      "|     0|       shelley|  1|\n",
      "|     0|            by|  1|\n",
      "|     1|         ebook|  1|\n",
      "|     1|        anyone|  1|\n",
      "|     1|           and|  1|\n",
      "|     1|            in|  1|\n",
      "|     1|            is|  1|\n",
      "|     1|           use|  1|\n",
      "|     1|           the|  2|\n",
      "|     1|        united|  1|\n",
      "|     1|      anywhere|  1|\n",
      "+------+--------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "by_words_tf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6737"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calulate total number of docs\n",
    "N_docs = by_words.count()\n",
    "N_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate document frequency\n",
    "by_words_df = by_words_count\\\n",
    "       .groupBy(\"word\")\\\n",
    "       .agg(countDistinct(\"doc_id\").alias(\"df\"))\\\n",
    "       .orderBy(F.col('df').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| word|  df|\n",
      "+-----+----+\n",
      "|  the|3284|\n",
      "|  and|2706|\n",
      "|   of|2436|\n",
      "|    i|2361|\n",
      "|   to|1901|\n",
      "|   my|1536|\n",
      "|    a|1315|\n",
      "|   in|1128|\n",
      "| that| 975|\n",
      "|  was| 949|\n",
      "|   me| 799|\n",
      "| with| 695|\n",
      "|  but| 683|\n",
      "|  had| 649|\n",
      "|which| 554|\n",
      "|   he| 552|\n",
      "|  you| 552|\n",
      "|   it| 534|\n",
      "|  not| 520|\n",
      "|  for| 507|\n",
      "+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "by_words_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for calulating IDF\n",
    "def calcIDF(df):\n",
    "    IDF = np.log(N_docs/df)\n",
    "    return IDF\n",
    "calcIDF_udf = udf(lambda z: calcIDF(z).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function to the dataframe\n",
    "by_words_idf = by_words_df\\\n",
    "       .withColumn('idf', calcIDF_udf(F.col(\"df\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------------+\n",
      "| word|  df|               idf|\n",
      "+-----+----+------------------+\n",
      "|  the|3284| 0.718552530388341|\n",
      "|  and|2706|0.9121431922299266|\n",
      "|   of|2436|1.0172573721308722|\n",
      "|    i|2361| 1.048529463875147|\n",
      "|   to|1901|1.2652346584722307|\n",
      "|   my|1536|1.4784330872530422|\n",
      "|    a|1315| 1.633778056348795|\n",
      "|   in|1128|1.7871685689026557|\n",
      "| that| 975|1.9329325299628126|\n",
      "|  was| 949| 1.959961202350732|\n",
      "|   me| 799| 2.132009055194385|\n",
      "| with| 695|2.2714581553958677|\n",
      "|  but| 683|  2.28887514138987|\n",
      "|  had| 649|2.3399372842565698|\n",
      "|which| 554| 2.498205314213376|\n",
      "|  you| 552|2.5018219546835647|\n",
      "|   he| 552|2.5018219546835647|\n",
      "|   it| 534| 2.534974162000465|\n",
      "|  not| 520|2.5615411893851867|\n",
      "|  for| 507|2.5868589973694767|\n",
      "+-----+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "by_words_idf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate tf-idf for each word\n",
    "by_words_tfidf = by_words_tf.select('doc_id', 'word', 'tf')\\\n",
    "       .join (by_words_idf, 'word')\\\n",
    "       .orderBy(F.col('doc_id').asc())\\\n",
    "       .withColumn(\"tf_idf\", F.col(\"tf\") * F.col(\"idf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+---+----+------------------+------------------+\n",
      "|          word|doc_id| tf|  df|               idf|            tf_idf|\n",
      "+--------------+------+---+----+------------------+------------------+\n",
      "|         ebook|     0|  1|  13| 6.250420643499123| 6.250420643499123|\n",
      "|       shelley|     0|  1|   3|  7.71675771229255|  7.71675771229255|\n",
      "|          mary|     0|  1|   3|  7.71675771229255|  7.71675771229255|\n",
      "|           the|     0|  1|3284| 0.718552530388341| 0.718552530388341|\n",
      "|            by|     0|  1| 480| 2.641583897058723| 2.641583897058723|\n",
      "|            of|     0|  1|2436|1.0172573721308722|1.0172573721308722|\n",
      "|       project|     0|  1|  88| 4.338033186482454| 4.338033186482454|\n",
      "|     gutenberg|     0|  1|  96| 4.251021809492824| 4.251021809492824|\n",
      "|wollstonecraft|     0|  1|   3|  7.71675771229255|  7.71675771229255|\n",
      "|        godwin|     0|  1|   3|  7.71675771229255|  7.71675771229255|\n",
      "|  frankenstein|     0|  1|  31| 5.381382796475513| 5.381382796475513|\n",
      "|        united|     1|  1|  20| 5.819637727406669| 5.819637727406669|\n",
      "|           use|     1|  1|  23|  5.67987578503151|  5.67987578503151|\n",
      "|         ebook|     1|  1|  13| 6.250420643499123| 6.250420643499123|\n",
      "|           for|     1|  1| 507|2.5868589973694767|2.5868589973694767|\n",
      "|      anywhere|     1|  1|   2| 8.122222820400715| 8.122222820400715|\n",
      "|        anyone|     1|  1|   9| 6.618145423624441| 6.618145423624441|\n",
      "|            in|     1|  1|1128|1.7871685689026557|1.7871685689026557|\n",
      "|            is|     1|  1| 305|3.0950582243532483|3.0950582243532483|\n",
      "|           and|     1|  1|2706|0.9121431922299266|0.9121431922299266|\n",
      "+--------------+------+---+----+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "by_words_tfidf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tf-idf for each document(row), create final dataframe\n",
    "by_words_final = by_words_tfidf\\\n",
    "       .groupBy(\"doc_id\").sum(\"tf_idf\")\\\n",
    "       .withColumnRenamed(\"sum(tf_idf)\", 'tf_idf vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|doc_id|     tf_idf vector|\n",
      "+------+------------------+\n",
      "|     0| 55.46528308469805|\n",
      "|     1| 51.91862506843507|\n",
      "|     2| 57.02242496716091|\n",
      "|     3| 67.08770598551247|\n",
      "|     4|48.723213844898986|\n",
      "|     5| 52.97018550081891|\n",
      "|     6|49.338885018292586|\n",
      "|     7|15.841680726319993|\n",
      "|     8|13.503605616876229|\n",
      "|     9|18.489440477629174|\n",
      "|    10|  37.8906413809028|\n",
      "|    11| 20.99078888752428|\n",
      "|    12| 29.33189086045275|\n",
      "|    13|12.437783151031109|\n",
      "|    14| 30.13751602443938|\n",
      "|    15|117.46291648866071|\n",
      "|    16| 43.13491496340592|\n",
      "|    17|29.673426050761677|\n",
      "|    18| 5.381382796475513|\n",
      "|    19|18.489440477629174|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "by_words_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that result is the dataframe\n",
    "type(by_words_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg eBook of Frankenstein, by Mary Wollstonecraft (Godwin) Shelley',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere in the United States and',\n",
       " 'most other parts of the world at no cost and with almost no restrictions',\n",
       " 'whatsoever. You may copy it, give it away or re-use it under the terms']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read text as RDD\n",
    "filepath = \"file:///home/jovyan/shared/lectures_folder/84-0.txt\"\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "RDD = sc.textFile(f\"{filepath}\")\n",
    "\n",
    "RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower letters, create a list of word in each row, delete empty rows\n",
    "RDD_by_words = RDD\\\n",
    "       .map(lambda text: process_string(text))\\\n",
    "       .filter(lambda x: len(x)>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'project',\n",
       "  'gutenberg',\n",
       "  'ebook',\n",
       "  'of',\n",
       "  'frankenstein',\n",
       "  'by',\n",
       "  'mary',\n",
       "  'wollstonecraft',\n",
       "  'godwin',\n",
       "  'shelley'],\n",
       " ['this',\n",
       "  'ebook',\n",
       "  'is',\n",
       "  'for',\n",
       "  'the',\n",
       "  'use',\n",
       "  'of',\n",
       "  'anyone',\n",
       "  'anywhere',\n",
       "  'in',\n",
       "  'the',\n",
       "  'united',\n",
       "  'states',\n",
       "  'and'],\n",
       " ['most',\n",
       "  'other',\n",
       "  'parts',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world',\n",
       "  'at',\n",
       "  'no',\n",
       "  'cost',\n",
       "  'and',\n",
       "  'with',\n",
       "  'almost',\n",
       "  'no',\n",
       "  'restrictions'],\n",
       " ['whatsoever',\n",
       "  'you',\n",
       "  'may',\n",
       "  'copy',\n",
       "  'it',\n",
       "  'give',\n",
       "  'it',\n",
       "  'away',\n",
       "  'or',\n",
       "  're',\n",
       "  'use',\n",
       "  'it',\n",
       "  'under',\n",
       "  'the',\n",
       "  'terms'],\n",
       " ['of',\n",
       "  'the',\n",
       "  'project',\n",
       "  'gutenberg',\n",
       "  'license',\n",
       "  'included',\n",
       "  'with',\n",
       "  'this',\n",
       "  'ebook',\n",
       "  'or',\n",
       "  'online',\n",
       "  'at'],\n",
       " ['www',\n",
       "  'gutenberg',\n",
       "  'org',\n",
       "  'if',\n",
       "  'you',\n",
       "  'are',\n",
       "  'not',\n",
       "  'located',\n",
       "  'in',\n",
       "  'the',\n",
       "  'united',\n",
       "  'states',\n",
       "  'you'],\n",
       " ['will',\n",
       "  'have',\n",
       "  'to',\n",
       "  'check',\n",
       "  'the',\n",
       "  'laws',\n",
       "  'of',\n",
       "  'the',\n",
       "  'country',\n",
       "  'where',\n",
       "  'you',\n",
       "  'are',\n",
       "  'located',\n",
       "  'before'],\n",
       " ['using', 'this', 'ebook'],\n",
       " ['title', 'frankenstein'],\n",
       " ['or', 'the', 'modern', 'prometheus']]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_by_words.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of rows are equal for both methods\n"
     ]
    }
   ],
   "source": [
    "#check if the number of rows equals to the result of part 1 calculation\n",
    "if RDD_by_words.count() == N_docs:\n",
    "    print('Numbers of rows are equal for both methods')\n",
    "else:\n",
    "    print('Numbers of rows differ by', RDD_by_words.count() - N_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#introduce indices for each document\n",
    "RDD_by_words = RDD_by_words.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['the',\n",
       "   'project',\n",
       "   'gutenberg',\n",
       "   'ebook',\n",
       "   'of',\n",
       "   'frankenstein',\n",
       "   'by',\n",
       "   'mary',\n",
       "   'wollstonecraft',\n",
       "   'godwin',\n",
       "   'shelley'],\n",
       "  0),\n",
       " (['this',\n",
       "   'ebook',\n",
       "   'is',\n",
       "   'for',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'anyone',\n",
       "   'anywhere',\n",
       "   'in',\n",
       "   'the',\n",
       "   'united',\n",
       "   'states',\n",
       "   'and'],\n",
       "  1),\n",
       " (['most',\n",
       "   'other',\n",
       "   'parts',\n",
       "   'of',\n",
       "   'the',\n",
       "   'world',\n",
       "   'at',\n",
       "   'no',\n",
       "   'cost',\n",
       "   'and',\n",
       "   'with',\n",
       "   'almost',\n",
       "   'no',\n",
       "   'restrictions'],\n",
       "  2),\n",
       " (['whatsoever',\n",
       "   'you',\n",
       "   'may',\n",
       "   'copy',\n",
       "   'it',\n",
       "   'give',\n",
       "   'it',\n",
       "   'away',\n",
       "   'or',\n",
       "   're',\n",
       "   'use',\n",
       "   'it',\n",
       "   'under',\n",
       "   'the',\n",
       "   'terms'],\n",
       "  3),\n",
       " (['of',\n",
       "   'the',\n",
       "   'project',\n",
       "   'gutenberg',\n",
       "   'license',\n",
       "   'included',\n",
       "   'with',\n",
       "   'this',\n",
       "   'ebook',\n",
       "   'or',\n",
       "   'online',\n",
       "   'at'],\n",
       "  4)]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_by_words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply map function to list all instances of each word in the doc\n",
    "RDD_tf = RDD_by_words.flatMap(lambda x: [((x[1],i),1) for i in x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 'the'), 1),\n",
       " ((0, 'project'), 1),\n",
       " ((0, 'gutenberg'), 1),\n",
       " ((0, 'ebook'), 1),\n",
       " ((0, 'of'), 1),\n",
       " ((0, 'frankenstein'), 1),\n",
       " ((0, 'by'), 1),\n",
       " ((0, 'mary'), 1),\n",
       " ((0, 'wollstonecraft'), 1),\n",
       " ((0, 'godwin'), 1)]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_tf.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply reduce function to count all instances of each word in each doc\n",
    "RDD_tf=RDD_tf.reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 'project'), 1),\n",
       " ((0, 'gutenberg'), 1),\n",
       " ((0, 'ebook'), 1),\n",
       " ((0, 'of'), 1),\n",
       " ((0, 'mary'), 1),\n",
       " ((0, 'shelley'), 1),\n",
       " ((1, 'for'), 1),\n",
       " ((1, 'the'), 2),\n",
       " ((1, 'states'), 1),\n",
       " ((1, 'and'), 1)]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_tf.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', (0, 1)),\n",
       " ('gutenberg', (0, 1)),\n",
       " ('ebook', (0, 1)),\n",
       " ('of', (0, 1)),\n",
       " ('mary', (0, 1))]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recombine previous rdd to (word - (doc_id - tf))\n",
    "RDD_2=RDD_tf.map(lambda x: (x[0][1],(x[0][0],x[1])))\n",
    "RDD_2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 1), ('gutenberg', 1), ('ebook', 1), ('of', 1), ('mary', 1)]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create an auxiliary rdd (word - 1), where each row is an occurence of a certain word in different documents\n",
    "RDD_3=RDD_2.map(lambda x: (x[0],1))\n",
    "RDD_3.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create rdd to calculate document frequency of each word\n",
    "RDD_df=RDD_3\\\n",
    "       .reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3284),\n",
       " ('and', 2706),\n",
       " ('of', 2436),\n",
       " ('i', 2361),\n",
       " ('to', 1901),\n",
       " ('my', 1536),\n",
       " ('a', 1315),\n",
       " ('in', 1128),\n",
       " ('that', 975),\n",
       " ('was', 949)]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_df.takeOrdered(10, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 4.338033186482454),\n",
       " ('gutenberg', 4.251021809492824),\n",
       " ('ebook', 6.250420643499123),\n",
       " ('of', 1.0172573721308722),\n",
       " ('mary', 7.71675771229255),\n",
       " ('shelley', 7.71675771229255),\n",
       " ('other', 4.293581423911619),\n",
       " ('world', 4.965222399250601),\n",
       " ('at', 3.0533186181804832),\n",
       " ('no', 3.673706444458)]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate idf for each word\n",
    "idf=RDD_df.map(lambda x: (x[0], np.log(N_docs/x[1])))\n",
    "idf.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.718552530388341),\n",
       " ('and', 0.9121431922299266),\n",
       " ('of', 1.0172573721308722),\n",
       " ('i', 1.048529463875147),\n",
       " ('to', 1.2652346584722307),\n",
       " ('my', 1.4784330872530422),\n",
       " ('a', 1.633778056348795),\n",
       " ('in', 1.7871685689026557),\n",
       " ('that', 1.9329325299628126),\n",
       " ('was', 1.959961202350732)]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf.takeOrdered(10, key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxiliary rdd, join 2 rdds, format of the final rdd (word - (doc_id - tf) - idf)\n",
    "RDD_4 =RDD_2.join(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gutenberg', ((0, 1), 4.251021809492824)),\n",
       " ('gutenberg', ((4, 1), 4.251021809492824)),\n",
       " ('gutenberg', ((6440, 1), 4.251021809492824)),\n",
       " ('gutenberg', ((6450, 1), 4.251021809492824)),\n",
       " ('gutenberg', ((6462, 1), 4.251021809492824)),\n",
       " ('gutenberg', ((6464, 1), 4.251021809492824)),\n",
       " ('gutenberg', ((6468, 1), 4.251021809492824)),\n",
       " ('gutenberg', ((6472, 1), 4.251021809492824)),\n",
       " ('gutenberg', ((6486, 1), 4.251021809492824)),\n",
       " ('gutenberg', ((6490, 1), 4.251021809492824))]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_4.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ('gutenberg', 1, 4.251021809492824, 4.251021809492824)),\n",
       " (0, ('of', 1, 1.0172573721308722, 1.0172573721308722)),\n",
       " (0, ('mary', 1, 7.71675771229255, 7.71675771229255)),\n",
       " (0, ('shelley', 1, 7.71675771229255, 7.71675771229255)),\n",
       " (0, ('the', 1, 0.718552530388341, 0.718552530388341))]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#auxiliary RDD to recombine rdd into format(doc_id - word - tf - idf - tf*idf)\n",
    "RDD_5=RDD_4.map(lambda x: (x[1][0][0],(x[0],x[1][0][1],x[1][1],x[1][0][1]*x[1][1]))).sortByKey()\n",
    "RDD_5.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4.251021809492824),\n",
       " (0, 1.0172573721308722),\n",
       " (0, 7.71675771229255),\n",
       " (0, 7.71675771229255),\n",
       " (0, 0.718552530388341),\n",
       " (0, 2.641583897058723),\n",
       " (0, 7.71675771229255),\n",
       " (0, 4.338033186482454),\n",
       " (0, 6.250420643499123),\n",
       " (0, 5.381382796475513)]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#auxiliary rdd, exclude all info apart from doc_id and tf-idf\n",
    "RDD_6 = RDD_5.map(lambda x: (x[0], x[1][3]))\n",
    "RDD_6.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 55.46528308469805),\n",
       " (1, 51.91862506843506),\n",
       " (2, 57.022424967160916),\n",
       " (3, 67.08770598551247),\n",
       " (4, 48.72321384489898),\n",
       " (5, 52.97018550081891),\n",
       " (6, 49.338885018292586),\n",
       " (7, 15.841680726319993),\n",
       " (8, 13.503605616876229),\n",
       " (9, 18.489440477629174)]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final RDD with doc_id and tf-idf vector for each doc\n",
    "RDD_final=RDD_6\\\n",
    "      .reduceByKey(lambda x,y:x+y)\\\n",
    "      .sortByKey()\n",
    "RDD_final.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that the type of output is RDD\n",
    "type(RDD_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
